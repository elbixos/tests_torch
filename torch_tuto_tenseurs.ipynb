{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quelques tests sur les tenseurs\n",
        "\n",
        "\n",
        "Bon, que se passe-t-il si on fait une multiplication matricielle avec des tenseurs de differentes tailles."
      ],
      "metadata": {
        "id": "9wMHWM-QnqP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Déja, on fait les install et les imports"
      ],
      "metadata": {
        "id": "d40F56BmoQ3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyBnVN2RepmM",
        "outputId": "106ebf07-b2f9-4afb-a596-80fc4590f065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchviz) (2.9.0+cu126)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchviz) (3.0.3)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "import einops\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "iEtoKo0Ae1aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplication de vecteurs\n",
        "\n",
        "Commençons par deux vecteurs (des tenseurs d'ordre 1).\n",
        "Le produit matriciel de deux vecteurs est (dans torch), un produit scalaire\n",
        "\n",
        "Une premiere remarque :\n",
        "**Un tenseur vectoriel n'a pas de direction,\n",
        "Il n'y a pas de vecteur ligne / vecteur colonne. Il n'y a que des tenseurs d'ordre 1**\n",
        "\n",
        "\n",
        "### Preparation des données :\n",
        "\n",
        "- 2 vecteurs : $x1$ et $x2$ de taille 3\n",
        "- 1 vecteur : $xt1$ de taille 2 (servira plus loin)"
      ],
      "metadata": {
        "id": "FjolRqBknxh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([1,2,3])\n",
        "x2 = np.array([2,3,4])\n",
        "\n",
        "xt1 = np.array([1,1])\n",
        "\n",
        "x1_tensor = torch.from_numpy(x1).float().to(device)\n",
        "x2_tensor = torch.from_numpy(x2).float().to(device)\n",
        "xt1_tensor = torch.from_numpy(xt1).float().to(device)\n",
        "\n",
        "print(x1)\n",
        "\n",
        "print(\"x1\",x1_tensor)\n",
        "print(\"x2\",x2_tensor)\n",
        "print(\"xt1\",xt1_tensor)\n",
        "\n",
        "x1_tensor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQu_UtyCeyAx",
        "outputId": "64d8b2f2-7a47-4fac-ad5a-6c7826399773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n",
            "x1 tensor([1., 2., 3.], device='cuda:0')\n",
            "x2 tensor([2., 3., 4.], device='cuda:0')\n",
            "xt1 tensor([1., 1.], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calcul du produit"
      ],
      "metadata": {
        "id": "E_ANpqkvpKBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Produit de deux vecteurs : produit scalaire\n",
        "prod = x1_tensor @ x2_tensor\n",
        "print(prod)\n",
        "\n",
        "# on peut récuperer un float\n",
        "res = prod.item()\n",
        "print(res, type(res))\n",
        "\n",
        "print(\"devrait etre 20\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0Btk-Ybjaqd",
        "outputId": "449a72f8-2f4e-4faf-9b64-1ca8319d68ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(20., device='cuda:0')\n",
            "20.0 <class 'float'>\n",
            "devrait etre 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pour comparaison, si on considérait des matrices, il faudrait :\n",
        "\n",
        "une matrice ligne x une matrice colonne\n",
        "\n",
        "comme suit"
      ],
      "metadata": {
        "id": "hLRsbIYFoh1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unsqueeze\n",
        "\n",
        "print(x1_tensor.unsqueeze(0))\n",
        "\n",
        "# on unsqueeze la derniere dimension\n",
        "print(x2_tensor.unsqueeze(-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1Kif04thyts",
        "outputId": "f9fb6501-f1bf-4900-f2d8-f370c34c6160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.]], device='cuda:0')\n",
            "tensor([[2.],\n",
            "        [3.],\n",
            "        [4.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiplication vecteur vecteur sous forme de deux matrices\n",
        "prod = x1_tensor.unsqueeze(0) @ x2_tensor.unsqueeze(-1)\n",
        "print(prod)\n",
        "\n",
        "# on peut récuperer un float\n",
        "res = prod[0][0].item()\n",
        "print(res, type(res))\n",
        "\n",
        "print(\"devrait etre 20\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwa9ry5Ygb1U",
        "outputId": "f4bf0a1d-ced6-41cc-de11-8b60b80937dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[20.]], device='cuda:0')\n",
            "20.0 <class 'float'>\n",
            "devrait etre 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplication matrice vecteur\n",
        "\n",
        "Un tenseur vectoriel n'a pas de direction,\n",
        "Il n'y a pas de vecteur ligne / vecteur colonne. Il n'y a que des tenseurs d'ordre 1\n",
        "\n",
        "il faut juste que le nombre de colonnes de la matrice soit la taille du vecteur.\n",
        "\n",
        "A part ca, pas de surprises.\n"
      ],
      "metadata": {
        "id": "ltYyaCrFpPVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M = np.array([[1,2,3],[1,1,1]])\n",
        "\n",
        "\n",
        "M_tensor = torch.from_numpy(M).float().to(device)\n",
        "print (M_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFxwtZ3di2Cl",
        "outputId": "5a83e5f9-bba8-4c4c-8b74-9f531e75554f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [1., 1., 1.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiplication Matrice vecteur\n",
        "\n",
        "print(M_tensor,\"\\n MULTIPLIE PAR \\n\",x2_tensor)\n",
        "\n",
        "prod = M_tensor @ x2_tensor\n",
        "print(\"\\n resu\\n\",prod)\n",
        "\n",
        "print(\"========================\")\n",
        "print(xt1_tensor,\"\\n MULTIPLIE PAR \\n\",M_tensor)\n",
        "\n",
        "prod = xt1_tensor @ M_tensor\n",
        "print(\"\\n resu\\n\",prod)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzhObdMbjVyT",
        "outputId": "1046b1a4-a227-4365-c956-31662f65be9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [1., 1., 1.]], device='cuda:0') \n",
            " MULTIPLIE PAR \n",
            " tensor([2., 3., 4.], device='cuda:0')\n",
            "\n",
            " resu\n",
            " tensor([20.,  9.], device='cuda:0')\n",
            "========================\n",
            "tensor([1., 1.], device='cuda:0') \n",
            " MULTIPLIE PAR \n",
            " tensor([[1., 2., 3.],\n",
            "        [1., 1., 1.]], device='cuda:0')\n",
            "\n",
            " resu\n",
            " tensor([2., 3., 4.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplication matrice matrice\n",
        "\n",
        "configuration typique d'un réseau de neurone traditionnel (dans ma tête)\n",
        "\n",
        "- On a un vecteur par exemple, de shape $(n,)$\n",
        "- on batche les vecteurs par paquets de taille $s_{batch}$\n",
        "- ca nous fait une matrice $X$ de shape $(n,s_{batch})$ (**chaque X est une colonne**)\n",
        "- on a une matrice de poids de shape $(n_{out},n)$\n",
        "- on fait le calcul suivant : $out = X \\times X + b$\n",
        "\n",
        "\n",
        "Ca, c'est dans ma tête ou sur mes dessins, parce que je fais toujours des multiplications matricielles à droite.\n",
        "\n",
        "\n",
        "### Remarque importante\n",
        "\n",
        "**EN VRAI : On batche sur la premiere dimension**\n",
        "\n",
        "- On a un vecteur par exemple, de shape $(n,)$\n",
        "- on batche les vecteurs pour en faire une matrice $X$ de shape $(s_{batch},n)$ (chaque $x$ est **une ligne**)\n",
        "- on a une matrice de poids de shape $(n_{out},n)$\n",
        "- on fait le calcul suivant : $out = X \\times W^\\intercal + b$\n",
        "\n",
        "$out$ est de shape $(s_{batch},n)$\n",
        "\n",
        "### Revenons à nos multiplications\n",
        "\n",
        "Ici, pas de surprise.\n",
        "\n",
        "Pour les tests, on va concatener les 2 vecteurs $x1$, $x2$ pour faire une matrice 3x2 $M1$ (comme dans ma tête)\n",
        "\n",
        "On calculera alors M x M1\n",
        "\n",
        "- comme on veut créer une nouvelle dimension (on passe de dim 1 à dim 2),\n",
        "on utilise *torch.stack*, pas *torch.cat*\n",
        "- comme on veut rester coherent dans le $ M \\times M1$, on les stack pour que les x initiaux soient \"en colonne\". La nouvelle direction est donc 1.\n",
        "\n",
        "J'ai encore batché dans la derniere dimension...\n"
      ],
      "metadata": {
        "id": "0pdAwg2GrIeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M1 = torch.stack((x1_tensor, x2_tensor),dim=1)\n",
        "\n",
        "print(M1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4c2yeVnrllT",
        "outputId": "a5b86985-ba72-4bc4-cfa6-066f98edf500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [2., 3.],\n",
            "        [3., 4.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prod = M_tensor @ M1\n",
        "\n",
        "print(M_tensor,\"\\n MULTIPLIE PAR \\n\",M1)\n",
        "\n",
        "prod = M_tensor @ M1\n",
        "print(\"\\n resu\\n\",prod)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqobonDus3BL",
        "outputId": "47b67ab2-52c5-472c-c5ad-633aaf95789f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [1., 1., 1.]], device='cuda:0') \n",
            " MULTIPLIE PAR \n",
            " tensor([[1., 2.],\n",
            "        [2., 3.],\n",
            "        [3., 4.]], device='cuda:0')\n",
            "\n",
            " resu\n",
            " tensor([[14., 20.],\n",
            "        [ 6.,  9.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrice x tenseur d'ordre 3\n",
        "\n",
        "Ici, on commence ce qui m'intéresse, car c'est ce qui se passe quand on passe\n",
        "une phrase dans un transformer.\n",
        "\n",
        "- chaque mot (token) est encodé en un vecteur de shape $(s_{token},)$\n",
        "- Une phrase est une sequence de token. c'est une matrice de shape $(s_{seq},s_{token})$\n",
        "- un batch de phrase est un tenseur de shape $(s_{batch},s_{seq},s_{token})$\n",
        "\n",
        "Prenons un MLP qui travaillerait sur chaque token indépendamment,\n",
        "pour chaque composante du token, il calcule une nouvelle sortie.\n",
        "\n",
        "Sa matrice est donc de shape $(s_{token},s_{token})$\n",
        "\n",
        "- Si on applique cette matrice à un token (vecteur), la sortie est un vecteur, tout se passe comme prévu.\n",
        "- Si on applique cette matrice à une sequence de token (une matrice), la sortie est le resultat d'une multiplication matricielle. Il faut **faire attention à ne pas faire d'opérations entre composantes de tokens différents**.\n",
        "\n",
        "\n",
        "Comme le montrent les exemples ci dessous, il faut faire $input \\times Matrice$\n",
        "plutot que $Matrice \\times input$ pour avoir le comportement attendu :\n",
        "\n",
        "quand on ajoute une nouvelle dimension, on répéte simplement l'opération de la matrice sur cette nouvelle dimension.\n"
      ],
      "metadata": {
        "id": "Lwv_ZrPWv0D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prenons une matrice qui calcule, pour un token, la somme et la différence de ses composantes\n",
        "W = np.array([[1,1],[1,-1]])\n",
        "\n",
        "\n",
        "W = torch.from_numpy(W).float().to(device)\n",
        "print (\"W\",W)\n",
        "\n",
        "# le premier token de la premiere phrase est [1,2]. le second est [3,4]\n",
        "# le premier token de la seconde phrase est [5,6]. le second est [7,8]\n",
        "X = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
        "X = torch.from_numpy(X).float().to(device)\n",
        "print(\"\\nX.shape\",X.shape)\n",
        "print (\"\\nX\\n\",X)\n",
        "\n",
        "\n",
        "print (\"\\n==========MULTIPLICATIONS PAR LA DROITE ===============\\n\")\n",
        "## Ceci fait ce que je pense : W x [1,2] -> (3, -1)\n",
        "print(\"\\n X[0][0]\\n\",X[0][0])\n",
        "res = W @ X[0][0]\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "## Voyons ceci  : W x [[1,2],[3,4]]\n",
        "# visiblement, ca fait la somme sur les colonnes, et la différence des colonnes. [4,6][-2,-2])\n",
        "print(\"\\n X[0]\\n\",X[0])\n",
        "res = W @ X[0]\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "## Voyons ceci  : W x [[1,2],[3,4]],[[5,6],[7,8]]\n",
        "# visiblement, ca fait comme avant, mais plusieurs fois\n",
        "print(\"\\n X\\n\",X)\n",
        "res = W @ X\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "print (\"\\n==========MULTIPLICATIONS PAR LA GAUCHE ===============\\n\")\n",
        "## Ceci fait ce que je pense : [1,2] x [1, 1],[1,-1]  -> (3, -1)\n",
        "print(\"\\n X[0][0]\\n\",X[0][0])\n",
        "res = X[0][0] @ W\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "## ceci fait bien la somme et la différence des composantes de chaque token\n",
        "## d'une phrase\n",
        "print(\"\\n X[0]\\n\",X[0])\n",
        "res = X[0] @ W\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "## ceci fait bien la somme et la différence des composantes de chaque token\n",
        "# de chaque phrase\n",
        "print(\"\\n X\\n\",X)\n",
        "res = X @ W\n",
        "print(\"\\nres\\n\",res)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFuMB_ViDDp0",
        "outputId": "3ae82d7f-947f-476e-d034-aeba75297f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W tensor([[ 1.,  1.],\n",
            "        [ 1., -1.]], device='cuda:0')\n",
            "\n",
            "X.shape torch.Size([2, 2, 2])\n",
            "\n",
            "X\n",
            " tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]], device='cuda:0')\n",
            "\n",
            "==========MULTIPLICATIONS PAR LA DROITE ===============\n",
            "\n",
            "\n",
            " X[0][0]\n",
            " tensor([1., 2.], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([ 3., -1.], device='cuda:0')\n",
            "\n",
            " X[0]\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([[ 4.,  6.],\n",
            "        [-2., -2.]], device='cuda:0')\n",
            "\n",
            " X\n",
            " tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([[[ 4.,  6.],\n",
            "         [-2., -2.]],\n",
            "\n",
            "        [[12., 14.],\n",
            "         [-2., -2.]]], device='cuda:0')\n",
            "\n",
            "==========MULTIPLICATIONS PAR LA GAUCHE ===============\n",
            "\n",
            "\n",
            " X[0][0]\n",
            " tensor([1., 2.], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([ 3., -1.], device='cuda:0')\n",
            "\n",
            " X[0]\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([[ 3., -1.],\n",
            "        [ 7., -1.]], device='cuda:0')\n",
            "\n",
            " X\n",
            " tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([[[ 3., -1.],\n",
            "         [ 7., -1.]],\n",
            "\n",
            "        [[11., -1.],\n",
            "         [15., -1.]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"\\n==========BATCHS (PAR LA GAUCHE) ===============\\n\")\n",
        "batch = torch.stack((X,2*X,3*X))\n",
        "print(\"\\nun batch de 3 phrases\\n\",batch)\n",
        "\n",
        "res = batch @ W\n",
        "print(\"resultat\",res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jP82gmUWBXL",
        "outputId": "4ef91569-0870-41d6-f6ba-7f51be2523f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========BATCHS (PAR LA GAUCHE) ===============\n",
            "\n",
            "\n",
            "un batch de 3 phrases\n",
            " tensor([[[[ 1.,  2.],\n",
            "          [ 3.,  4.]],\n",
            "\n",
            "         [[ 5.,  6.],\n",
            "          [ 7.,  8.]]],\n",
            "\n",
            "\n",
            "        [[[ 2.,  4.],\n",
            "          [ 6.,  8.]],\n",
            "\n",
            "         [[10., 12.],\n",
            "          [14., 16.]]],\n",
            "\n",
            "\n",
            "        [[[ 3.,  6.],\n",
            "          [ 9., 12.]],\n",
            "\n",
            "         [[15., 18.],\n",
            "          [21., 24.]]]], device='cuda:0')\n",
            "resultat tensor([[[[ 3., -1.],\n",
            "          [ 7., -1.]],\n",
            "\n",
            "         [[11., -1.],\n",
            "          [15., -1.]]],\n",
            "\n",
            "\n",
            "        [[[ 6., -2.],\n",
            "          [14., -2.]],\n",
            "\n",
            "         [[22., -2.],\n",
            "          [30., -2.]]],\n",
            "\n",
            "\n",
            "        [[[ 9., -3.],\n",
            "          [21., -3.]],\n",
            "\n",
            "         [[33., -3.],\n",
            "          [45., -3.]]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INTERPRETATIONS DE CES MULTIPLICATIONS : BROADCASTING\n",
        "\n",
        "lu quelque part : \"The matrix multiplication(s) are done between the last two dimensions. The remaining first three dimensions are broadcast and are ‘batch’\"\n",
        "\n",
        "testons ca. Ici, on a\n",
        "\n",
        "- un tenseur $W_{batch}$ de shape $[2,2,2]$ qui représente 2 matrices empilées\n",
        "- un tenseur $batch$ de shape $[3,2,2,2]$ qui représente nos inputs. Cela pourrait être 3 images à deux canaux.\n",
        "\n",
        "Pour bien s'assurer de ce que l'on fait, vu les égalités de longueur des shape, précisons l'ordre des canaux pour $batch$ : $[B, C, H, W]$\n",
        "\n",
        "Pour la matrice, la shape est $[C, H, W]$\n",
        "\n",
        "On calcule $ batch \\times W_{batch}$.\n",
        "\n",
        "Le résultat est surprenant :\n",
        "\n",
        "- la premiere matrice de $W_{batch}$ multiplie le premier canal d'une image.\n",
        "- la seconde matrice de $W_{batch}$ multiplie le second canal d'une image.\n",
        "- ces opérations sont répétées pour chaque image\n",
        "\n",
        "\n",
        "\n",
        "Ceci est lié au fait que la multiplication **broadcaste** les données : https://docs.pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics\n",
        "\n",
        "le **broadcast** dans la multiplication matricielle a des comportements differents en fonction des tailles des dimensions respectives des tenseurs.\n",
        "\n",
        "Pour savoir s'il faut et si on peut broadcaster, **on regarde les dimensions en partant de la fin et en remontant vers le début**\n",
        "\n",
        "1. Pour *matmult* : les deux dernieres dimensions doivent être compatibles pour une multiplication matricielle.\n",
        "\n",
        "2. comme la troisieme dimension en partant de la fin est de même taille dans les deux tenseurs, tout se passe comme si on appliquait les calculs dans les dimensions suivantes de façon parallele. Ceci explique comment on ferait un calcul d'un tenseur de shape $[C,H,W] \\times$ un tenseur de shape $[C,H,W]$ :\n",
        "**chaque matrice $[H,W]$ des inputs est multipliée par la matrice $[H,W]$ correspondante du tenseur.**\n",
        "\n",
        "3. Pour la partie batch, la 4eme dimension en partant de la fin n'existe pas dans W. Pas de problème, le broadcast va la créer et dupliquer les données.\n",
        "Pour imaginer ca, disons qu'un scalaire $a$, ca peut se broadcaster en un vecteur $[a,a,a,a]$. **Notons que** $a$ **pourrait être un vecteur ou une matrice, ca marcherait pareil**. Donc le tenseur $W_{batch}$ va être étendu pour\n",
        "appliquer l'opération 2 pour chaque \"phrase\" de $batch$\n",
        "\n",
        "4. on s'en servira plus loin, mais une dimension de 1 peut aussi se broadcaster par duplication. Pour imaginer ca, disons qu'une matrice 1,2 telle que $[[1,2,3]]$ peut se broadcaster en une matrice 3,2 : $[[1,2,3],[1,2,3],[1,2,3]]$\n"
      ],
      "metadata": {
        "id": "aYAWrOD8aBZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on garde le X précédent, on prépare une liste de matrice.\n",
        "# W1 va conserver la premiere composante d'un token.\n",
        "W1 = torch.from_numpy(np.array([[1,0],[0,0]])).float().to(device)\n",
        "\n",
        "W_batch = torch.stack((W,W1))\n",
        "print(\"\\nW_batch\\n\",W_batch)\n",
        "\n",
        "res = batch @ W_batch\n",
        "\n",
        "print(\"\\nX\\n\",batch)\n",
        "\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "print(\"batch.shape\",batch.shape,\"W.shape\",W_batch.shape,\"res.shape :\",res.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnH_AxQ8bazs",
        "outputId": "2ff56aae-3e7d-4f24-cb03-dda83d51a01e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_batch\n",
            " tensor([[[ 1.,  1.],\n",
            "         [ 1., -1.]],\n",
            "\n",
            "        [[ 1.,  0.],\n",
            "         [ 0.,  0.]]], device='cuda:0')\n",
            "\n",
            "X\n",
            " tensor([[[[ 1.,  2.],\n",
            "          [ 3.,  4.]],\n",
            "\n",
            "         [[ 5.,  6.],\n",
            "          [ 7.,  8.]]],\n",
            "\n",
            "\n",
            "        [[[ 2.,  4.],\n",
            "          [ 6.,  8.]],\n",
            "\n",
            "         [[10., 12.],\n",
            "          [14., 16.]]],\n",
            "\n",
            "\n",
            "        [[[ 3.,  6.],\n",
            "          [ 9., 12.]],\n",
            "\n",
            "         [[15., 18.],\n",
            "          [21., 24.]]]], device='cuda:0')\n",
            "\n",
            "res\n",
            " tensor([[[[ 3., -1.],\n",
            "          [ 7., -1.]],\n",
            "\n",
            "         [[ 5.,  0.],\n",
            "          [ 7.,  0.]]],\n",
            "\n",
            "\n",
            "        [[[ 6., -2.],\n",
            "          [14., -2.]],\n",
            "\n",
            "         [[10.,  0.],\n",
            "          [14.,  0.]]],\n",
            "\n",
            "\n",
            "        [[[ 9., -3.],\n",
            "          [21., -3.]],\n",
            "\n",
            "         [[15.,  0.],\n",
            "          [21.,  0.]]]], device='cuda:0')\n",
            "batch.shape torch.Size([3, 2, 2, 2]) W.shape torch.Size([2, 2, 2]) res.shape : torch.Size([3, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadcasting reloaded\n",
        "\n",
        "Vu la doc, on va tester ceci : appliquer une deuxieme transformation à chacun de nos tokens.\n",
        "\n",
        "On va modifier les inputs pour creer une dimension juste avant les deux dernieres, pour avoir : $[B, C,1, H, W]$\n",
        "\n",
        "la matrice est de shape : [2,H,W]\n",
        "\n",
        "On calcule X @ W\n",
        "\n",
        "cette fois ci, chaque matrice H,W des données passe dans chacune des deux matrices de traitement.\n",
        "\n",
        "Le broadcasting a en fait dupliqué les données des dimensions suivantes dans la dimension de taille 1 pour égaler la taille 2. Puis on applique la strat précédente.\n",
        "\n",
        "**Ce sont des manipulations comme celle ci (et celles d'avant) qui permettent de faire de l'attention spatiale ou temporelle à moindre frais** : on va selectionner des vecteurs pertinents par permutation, pour les batcher.\n",
        "\n",
        "**a noter : dans un cadre opérationnel, vu les résultats ci dessous, il faudrait peut être permuter les dimensions du résultat** pour que les 2 calculs effectués sur chaque token soient la deuxieme dimension en partant de la fin.\n",
        "\n",
        "En l'état, à la sortie, j'ai une shape $[B,nb_{op},seq, embed]$\n"
      ],
      "metadata": {
        "id": "l4tjQNi61dCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_reshaped = batch.reshape(3,2,1,2,2)\n",
        "#print (\"\\nbatch reshaped\\n\",batch_reshaped)\n",
        "\n",
        "res = X_reshaped @ W_batch\n",
        "print(\"\\nres\\n\",res)\n",
        "\n",
        "print(\"X.shape\",X_reshaped.shape,\"W.shape\",W_batch.shape,\"res.shape :\",res.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoSszyJP17_E",
        "outputId": "cf65fe76-7fbd-49fe-a370-83923c9e69cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "res\n",
            " tensor([[[[[ 3., -1.],\n",
            "           [ 7., -1.]],\n",
            "\n",
            "          [[ 1.,  0.],\n",
            "           [ 3.,  0.]]],\n",
            "\n",
            "\n",
            "         [[[11., -1.],\n",
            "           [15., -1.]],\n",
            "\n",
            "          [[ 5.,  0.],\n",
            "           [ 7.,  0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 6., -2.],\n",
            "           [14., -2.]],\n",
            "\n",
            "          [[ 2.,  0.],\n",
            "           [ 6.,  0.]]],\n",
            "\n",
            "\n",
            "         [[[22., -2.],\n",
            "           [30., -2.]],\n",
            "\n",
            "          [[10.,  0.],\n",
            "           [14.,  0.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 9., -3.],\n",
            "           [21., -3.]],\n",
            "\n",
            "          [[ 3.,  0.],\n",
            "           [ 9.,  0.]]],\n",
            "\n",
            "\n",
            "         [[[33., -3.],\n",
            "           [45., -3.]],\n",
            "\n",
            "          [[15.,  0.],\n",
            "           [21.,  0.]]]]], device='cuda:0')\n",
            "X.shape torch.Size([3, 2, 1, 2, 2]) W.shape torch.Size([2, 2, 2]) res.shape : torch.Size([3, 2, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenseurs, reshape et flatten\n"
      ],
      "metadata": {
        "id": "aoznZIvpTQxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imR = np.array([[1,2,3,4],[5,6,7,8]])\n",
        "imG = np.array([[11,12,13,14],[15,16,17,18]])\n",
        "\n",
        "im = np.stack([imR,imG])\n",
        "\n",
        "im = torch.from_numpy(im).float().to(device)\n",
        "print(im)\n",
        "\n",
        "C = 2\n",
        "H = 2\n",
        "W = 4\n",
        "\n",
        "# On reshape en patch de taille 1x1...\n",
        "im_reshaped = im.reshape(2,H*W)\n",
        "print(\"\\n après reshape\\n\")\n",
        "print(im_reshaped)\n",
        "\n",
        "print(\"ci dessus, j'ai tous les pixels d'un canal en dimension finale.\")\n",
        "\n",
        "# Je permute pour avoir chaque patch en dimension finale.\n",
        "im_permute = im_reshaped.permute(1,0)\n",
        "print(im_permute)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD3Up9J6Ts2H",
        "outputId": "6066f83f-6656-43a1-c860-9d33fef2bd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.,  2.,  3.,  4.],\n",
            "         [ 5.,  6.,  7.,  8.]],\n",
            "\n",
            "        [[11., 12., 13., 14.],\n",
            "         [15., 16., 17., 18.]]], device='cuda:0')\n",
            "\n",
            " après reshape\n",
            "\n",
            "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
            "        [11., 12., 13., 14., 15., 16., 17., 18.]], device='cuda:0')\n",
            "ci dessus, j'ai tous les pixels d'un canal en dimension finale.\n",
            "tensor([[ 1., 11.],\n",
            "        [ 2., 12.],\n",
            "        [ 3., 13.],\n",
            "        [ 4., 14.],\n",
            "        [ 5., 15.],\n",
            "        [ 6., 16.],\n",
            "        [ 7., 17.],\n",
            "        [ 8., 18.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idem mais avec des patchs"
      ],
      "metadata": {
        "id": "kk4jv6PtW9OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "imR = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16],[17,18,19,20],[21,22,23,24]])\n",
        "imG = np.array([[101,102,103,104],[105,106,107,108],[109,110,111,112],[113,114,115,116],[117,118,119,120],[121,122,123,124]])\n",
        "\n",
        "im = np.stack([imR,imG])\n",
        "\n",
        "im = torch.from_numpy(im).float().to(device)\n",
        "print(im)\n",
        "\n",
        "C = 2\n",
        "H = 6\n",
        "W = 4\n",
        "\n",
        "# On reshape en patch de taille 1x2...\n",
        "\n",
        "sp_i = 3 # taille des patchs en nb lignes\n",
        "sp_j = 2 #taille des patchs en nb colonnes\n",
        "\n",
        "im_reshaped = im.reshape(C,H//sp_i,sp_i, W//sp_j,sp_j)\n",
        "print(\"\\n après reshape\\n\")\n",
        "print(im_reshaped)\n",
        "\n",
        "print (\"\\nregardons juste dans le canal rouge im_reshaped[0]\\n\")\n",
        "print(im_reshaped[0])\n",
        "print (\"\\nregardons le second patch de ligne des ligne du canal rouge im_reshaped[0][1]\")\n",
        "print(im_reshaped[0][1])\n",
        "\n",
        "\n",
        "print(\"ci dessus, j'ai bien tout ce qui correspond au deuxieme patch de ligne, découpé en patchs de colonnes\")\n",
        "print(\"le problème est que la dimension suivante est du numéro de ligne dans le patch.\")\n",
        "print(\"par exemple, 3eme ligne du deuxieme patch de ligne : im_reshaped[0][1][2]\\n\")\n",
        "print(im_reshaped[0][1][2])\n",
        "\n",
        "\n",
        "print(\"-------------------------\")\n",
        "\n",
        "# Je permute pour avoir chaque patch en dimension finale.\n",
        "im_permute = im_reshaped.permute(1,3,0,2,4)\n",
        "print(im_permute)\n",
        "\n",
        "print(\"ci dessus, c'est pas mal, j'ai reuni mes données de chaque patch sur les 3 dernieres dimensions\\n\")\n",
        "print(\"reste plus qu'a flatten le tout proprement\")\n",
        "\n",
        "# Je flatten les dimensions finales\n",
        "im_flattened_patches = im_permute.flatten(2,4)\n",
        "print(im_flattened_patches)\n",
        "print(\"\\nci dessus, j'ai une matrice 2x2 de vecteurs\\n\")\n",
        "\n",
        "# Je flatten les dimensions 0 et 1 correspondant aux numéros de patchs en i et j\n",
        "im_flattened = im_flattened_patches.flatten(0,1)\n",
        "\n",
        "print(im_flattened)\n",
        "\n",
        "print(\"\\nci dessus, j'ai bien une liste de 4 patchs de vecteurs\\n\")\n",
        "\n",
        "print(\"\\n shape des données : (4, 2x3x2) : \", im_flattened.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS26YNzeW5jw",
        "outputId": "39646dcf-426e-4206-9346-e4dfc86f567a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[  1.,   2.,   3.,   4.],\n",
            "         [  5.,   6.,   7.,   8.],\n",
            "         [  9.,  10.,  11.,  12.],\n",
            "         [ 13.,  14.,  15.,  16.],\n",
            "         [ 17.,  18.,  19.,  20.],\n",
            "         [ 21.,  22.,  23.,  24.]],\n",
            "\n",
            "        [[101., 102., 103., 104.],\n",
            "         [105., 106., 107., 108.],\n",
            "         [109., 110., 111., 112.],\n",
            "         [113., 114., 115., 116.],\n",
            "         [117., 118., 119., 120.],\n",
            "         [121., 122., 123., 124.]]], device='cuda:0')\n",
            "\n",
            " après reshape\n",
            "\n",
            "tensor([[[[[  1.,   2.],\n",
            "           [  3.,   4.]],\n",
            "\n",
            "          [[  5.,   6.],\n",
            "           [  7.,   8.]],\n",
            "\n",
            "          [[  9.,  10.],\n",
            "           [ 11.,  12.]]],\n",
            "\n",
            "\n",
            "         [[[ 13.,  14.],\n",
            "           [ 15.,  16.]],\n",
            "\n",
            "          [[ 17.,  18.],\n",
            "           [ 19.,  20.]],\n",
            "\n",
            "          [[ 21.,  22.],\n",
            "           [ 23.,  24.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[101., 102.],\n",
            "           [103., 104.]],\n",
            "\n",
            "          [[105., 106.],\n",
            "           [107., 108.]],\n",
            "\n",
            "          [[109., 110.],\n",
            "           [111., 112.]]],\n",
            "\n",
            "\n",
            "         [[[113., 114.],\n",
            "           [115., 116.]],\n",
            "\n",
            "          [[117., 118.],\n",
            "           [119., 120.]],\n",
            "\n",
            "          [[121., 122.],\n",
            "           [123., 124.]]]]], device='cuda:0')\n",
            "\n",
            "regardons juste dans le canal rouge im_reshaped[0]\n",
            "\n",
            "tensor([[[[ 1.,  2.],\n",
            "          [ 3.,  4.]],\n",
            "\n",
            "         [[ 5.,  6.],\n",
            "          [ 7.,  8.]],\n",
            "\n",
            "         [[ 9., 10.],\n",
            "          [11., 12.]]],\n",
            "\n",
            "\n",
            "        [[[13., 14.],\n",
            "          [15., 16.]],\n",
            "\n",
            "         [[17., 18.],\n",
            "          [19., 20.]],\n",
            "\n",
            "         [[21., 22.],\n",
            "          [23., 24.]]]], device='cuda:0')\n",
            "\n",
            "regardons le second patch de ligne des ligne du canal rouge im_reshaped[0][1]\n",
            "tensor([[[13., 14.],\n",
            "         [15., 16.]],\n",
            "\n",
            "        [[17., 18.],\n",
            "         [19., 20.]],\n",
            "\n",
            "        [[21., 22.],\n",
            "         [23., 24.]]], device='cuda:0')\n",
            "ci dessus, j'ai bien tout ce qui correspond au deuxieme patch de ligne, découpé en patchs de colonnes\n",
            "le problème est que la dimension suivante est du numéro de ligne dans le patch.\n",
            "par exemple, 3eme ligne du deuxieme patch de ligne : im_reshaped[0][1][2]\n",
            "\n",
            "tensor([[21., 22.],\n",
            "        [23., 24.]], device='cuda:0')\n",
            "-------------------------\n",
            "tensor([[[[[  1.,   2.],\n",
            "           [  5.,   6.],\n",
            "           [  9.,  10.]],\n",
            "\n",
            "          [[101., 102.],\n",
            "           [105., 106.],\n",
            "           [109., 110.]]],\n",
            "\n",
            "\n",
            "         [[[  3.,   4.],\n",
            "           [  7.,   8.],\n",
            "           [ 11.,  12.]],\n",
            "\n",
            "          [[103., 104.],\n",
            "           [107., 108.],\n",
            "           [111., 112.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[ 13.,  14.],\n",
            "           [ 17.,  18.],\n",
            "           [ 21.,  22.]],\n",
            "\n",
            "          [[113., 114.],\n",
            "           [117., 118.],\n",
            "           [121., 122.]]],\n",
            "\n",
            "\n",
            "         [[[ 15.,  16.],\n",
            "           [ 19.,  20.],\n",
            "           [ 23.,  24.]],\n",
            "\n",
            "          [[115., 116.],\n",
            "           [119., 120.],\n",
            "           [123., 124.]]]]], device='cuda:0')\n",
            "ci dessus, c'est pas mal, j'ai reuni mes données de chaque patch sur les 3 dernieres dimensions\n",
            "\n",
            "reste plus qu'a flatten le tout proprement\n",
            "tensor([[[  1.,   2.,   5.,   6.,   9.,  10., 101., 102., 105., 106., 109.,\n",
            "          110.],\n",
            "         [  3.,   4.,   7.,   8.,  11.,  12., 103., 104., 107., 108., 111.,\n",
            "          112.]],\n",
            "\n",
            "        [[ 13.,  14.,  17.,  18.,  21.,  22., 113., 114., 117., 118., 121.,\n",
            "          122.],\n",
            "         [ 15.,  16.,  19.,  20.,  23.,  24., 115., 116., 119., 120., 123.,\n",
            "          124.]]], device='cuda:0')\n",
            "\n",
            "ci dessus, j'ai une matrice 2x2 de vecteurs\n",
            "\n",
            "tensor([[  1.,   2.,   5.,   6.,   9.,  10., 101., 102., 105., 106., 109., 110.],\n",
            "        [  3.,   4.,   7.,   8.,  11.,  12., 103., 104., 107., 108., 111., 112.],\n",
            "        [ 13.,  14.,  17.,  18.,  21.,  22., 113., 114., 117., 118., 121., 122.],\n",
            "        [ 15.,  16.,  19.,  20.,  23.,  24., 115., 116., 119., 120., 123., 124.]],\n",
            "       device='cuda:0')\n",
            "\n",
            "ci dessus, j'ai bien une liste de 4 patchs de vecteurs\n",
            "\n",
            "\n",
            " shape des données : (4, 2x3x2) :  torch.Size([4, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### on code des video\n",
        "\n",
        "Bon. on va se faire une video de 2 images.\n",
        "chaque image a :\n",
        "- 2 canaux\n",
        "- une taille de 6x4\n",
        "\n",
        "la seconde image est juste l'opposé de la premiere.\n",
        "\n",
        "nos données ont une shape : (frames,C,H,W)\n",
        "\n",
        "Eventuellement, on pourra faire un batch de video en stackant $(vid, 2.vid)$\n",
        "\n",
        "on fait des patchs de taille $[tp,Hp,Wp] = [2,3,2]$\n",
        "\n",
        "ca fait 2x2 patchs"
      ],
      "metadata": {
        "id": "UIF-jAZzU8yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# une fonction pour faire les patchs\n",
        "def extract_patches(video,C = 2,H = 6,W = 4,frames=2, sp_i=3,sp_j=2):\n",
        "  im_reshaped = video.reshape(frames,C,H//sp_i,sp_i, W//sp_j,sp_j)\n",
        "\n",
        "  # Je permute pour avoir chaque patch en dimension finale.\n",
        "  # shape apres : [ni_patch, nj_patch, frames,C,Hp,Wp]\n",
        "  im_permute = im_reshaped.permute(2,4,0,1,3,5)\n",
        "  #print(im_permute)\n",
        "\n",
        "  # Je flatten les dimensions finales\n",
        "  #im_permute = im_permute.flatten(2,5)\n",
        "  #print(im_permute)\n",
        "  #print(\"\\nci dessus, j'ai une matrice 2x2 de vecteurs\\n\")\n",
        "\n",
        "  # Je flatten les dimensions 0 et 1 correspondant aux numéros de patchs en i et j\n",
        "  # shape après : [ni_patch * nj_patch, frames,C,Hp,Wp]\n",
        "  im_flattened = im_permute.flatten(0,1)\n",
        "\n",
        "  return im_flattened\n"
      ],
      "metadata": {
        "id": "TAvMaKBOYB1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vid = torch.stack((im,-im))\n",
        "batch_vid = torch.stack((vid,2*vid))\n",
        "\n",
        "print(vid.shape)\n",
        "print (\"\\nvid\\n\", vid)\n",
        "\n",
        "\n",
        "patches = extract_patches(vid)\n",
        "print(\"\\npatches.shape\\n\",patches.shape)\n",
        "print(\"\\n patches\\n\",patches)\n",
        "\n",
        "print(\"\\n=====preparation pour travail spatiotemporel=====\\n\")\n",
        "# shape avant : [ni_patch * nj_patch, frames,C,Hp,Wp]\n",
        "p_spatiotemp = patches.permute(1,0,2,3,4).reshape(2*4,-1)\n",
        "print(p_spatiotemp)\n",
        "print(\"\\np_spatiotemp.shape\\n\",p_spatiotemp.shape)\n",
        "print(\"PARFAIT\")\n",
        "\n",
        "\n",
        "print(\"\\n=====preparation pour travail spatial=====\\n\")\n",
        "# shape avant : [ni_patch * nj_patch, frames,C,Hp,Wp]\n",
        "p_spatial = patches.permute(1,0,2,3,4).reshape(2,4,-1)\n",
        "print(p_spatial)\n",
        "print(\"\\np_spatial.shape\\n\",p_spatial.shape)\n",
        "print(\"PARFAIT\")\n",
        "\n",
        "print(\"\\n =====preparation pour travail temporel====\\n\")\n",
        "# shape avant : [ni_patch * nj_patch, frames,C,Hp,Wp]\n",
        "p_temp = einops.rearrange(patches,\"np t c h w -> np h w (t c) \")\n",
        "print(p_temp)\n",
        "print(\"\\np_temp.shape\\n\",p_temp.shape)\n",
        "print(\"je ne sais pas si c'est PARFAIT\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDkOtqJ8VGcz",
        "outputId": "b3fdb6ef-4481-4bd6-80c1-16c7bbdb5805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 6, 4])\n",
            "\n",
            "vid\n",
            " tensor([[[[   1.,    2.,    3.,    4.],\n",
            "          [   5.,    6.,    7.,    8.],\n",
            "          [   9.,   10.,   11.,   12.],\n",
            "          [  13.,   14.,   15.,   16.],\n",
            "          [  17.,   18.,   19.,   20.],\n",
            "          [  21.,   22.,   23.,   24.]],\n",
            "\n",
            "         [[ 101.,  102.,  103.,  104.],\n",
            "          [ 105.,  106.,  107.,  108.],\n",
            "          [ 109.,  110.,  111.,  112.],\n",
            "          [ 113.,  114.,  115.,  116.],\n",
            "          [ 117.,  118.,  119.,  120.],\n",
            "          [ 121.,  122.,  123.,  124.]]],\n",
            "\n",
            "\n",
            "        [[[  -1.,   -2.,   -3.,   -4.],\n",
            "          [  -5.,   -6.,   -7.,   -8.],\n",
            "          [  -9.,  -10.,  -11.,  -12.],\n",
            "          [ -13.,  -14.,  -15.,  -16.],\n",
            "          [ -17.,  -18.,  -19.,  -20.],\n",
            "          [ -21.,  -22.,  -23.,  -24.]],\n",
            "\n",
            "         [[-101., -102., -103., -104.],\n",
            "          [-105., -106., -107., -108.],\n",
            "          [-109., -110., -111., -112.],\n",
            "          [-113., -114., -115., -116.],\n",
            "          [-117., -118., -119., -120.],\n",
            "          [-121., -122., -123., -124.]]]], device='cuda:0')\n",
            "\n",
            "patches.shape\n",
            " torch.Size([4, 2, 2, 3, 2])\n",
            "\n",
            " patches\n",
            " tensor([[[[[   1.,    2.],\n",
            "           [   5.,    6.],\n",
            "           [   9.,   10.]],\n",
            "\n",
            "          [[ 101.,  102.],\n",
            "           [ 105.,  106.],\n",
            "           [ 109.,  110.]]],\n",
            "\n",
            "\n",
            "         [[[  -1.,   -2.],\n",
            "           [  -5.,   -6.],\n",
            "           [  -9.,  -10.]],\n",
            "\n",
            "          [[-101., -102.],\n",
            "           [-105., -106.],\n",
            "           [-109., -110.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[   3.,    4.],\n",
            "           [   7.,    8.],\n",
            "           [  11.,   12.]],\n",
            "\n",
            "          [[ 103.,  104.],\n",
            "           [ 107.,  108.],\n",
            "           [ 111.,  112.]]],\n",
            "\n",
            "\n",
            "         [[[  -3.,   -4.],\n",
            "           [  -7.,   -8.],\n",
            "           [ -11.,  -12.]],\n",
            "\n",
            "          [[-103., -104.],\n",
            "           [-107., -108.],\n",
            "           [-111., -112.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[  13.,   14.],\n",
            "           [  17.,   18.],\n",
            "           [  21.,   22.]],\n",
            "\n",
            "          [[ 113.,  114.],\n",
            "           [ 117.,  118.],\n",
            "           [ 121.,  122.]]],\n",
            "\n",
            "\n",
            "         [[[ -13.,  -14.],\n",
            "           [ -17.,  -18.],\n",
            "           [ -21.,  -22.]],\n",
            "\n",
            "          [[-113., -114.],\n",
            "           [-117., -118.],\n",
            "           [-121., -122.]]]],\n",
            "\n",
            "\n",
            "\n",
            "        [[[[  15.,   16.],\n",
            "           [  19.,   20.],\n",
            "           [  23.,   24.]],\n",
            "\n",
            "          [[ 115.,  116.],\n",
            "           [ 119.,  120.],\n",
            "           [ 123.,  124.]]],\n",
            "\n",
            "\n",
            "         [[[ -15.,  -16.],\n",
            "           [ -19.,  -20.],\n",
            "           [ -23.,  -24.]],\n",
            "\n",
            "          [[-115., -116.],\n",
            "           [-119., -120.],\n",
            "           [-123., -124.]]]]], device='cuda:0')\n",
            "\n",
            "=====preparation pour travail spatiotemporel=====\n",
            "\n",
            "tensor([[   1.,    2.,    5.,    6.,    9.,   10.,  101.,  102.,  105.,  106.,\n",
            "          109.,  110.],\n",
            "        [   3.,    4.,    7.,    8.,   11.,   12.,  103.,  104.,  107.,  108.,\n",
            "          111.,  112.],\n",
            "        [  13.,   14.,   17.,   18.,   21.,   22.,  113.,  114.,  117.,  118.,\n",
            "          121.,  122.],\n",
            "        [  15.,   16.,   19.,   20.,   23.,   24.,  115.,  116.,  119.,  120.,\n",
            "          123.,  124.],\n",
            "        [  -1.,   -2.,   -5.,   -6.,   -9.,  -10., -101., -102., -105., -106.,\n",
            "         -109., -110.],\n",
            "        [  -3.,   -4.,   -7.,   -8.,  -11.,  -12., -103., -104., -107., -108.,\n",
            "         -111., -112.],\n",
            "        [ -13.,  -14.,  -17.,  -18.,  -21.,  -22., -113., -114., -117., -118.,\n",
            "         -121., -122.],\n",
            "        [ -15.,  -16.,  -19.,  -20.,  -23.,  -24., -115., -116., -119., -120.,\n",
            "         -123., -124.]], device='cuda:0')\n",
            "\n",
            "p_spatiotemp.shape\n",
            " torch.Size([8, 12])\n",
            "PARFAIT\n",
            "\n",
            "=====preparation pour travail spatial=====\n",
            "\n",
            "tensor([[[   1.,    2.,    5.,    6.,    9.,   10.,  101.,  102.,  105.,  106.,\n",
            "           109.,  110.],\n",
            "         [   3.,    4.,    7.,    8.,   11.,   12.,  103.,  104.,  107.,  108.,\n",
            "           111.,  112.],\n",
            "         [  13.,   14.,   17.,   18.,   21.,   22.,  113.,  114.,  117.,  118.,\n",
            "           121.,  122.],\n",
            "         [  15.,   16.,   19.,   20.,   23.,   24.,  115.,  116.,  119.,  120.,\n",
            "           123.,  124.]],\n",
            "\n",
            "        [[  -1.,   -2.,   -5.,   -6.,   -9.,  -10., -101., -102., -105., -106.,\n",
            "          -109., -110.],\n",
            "         [  -3.,   -4.,   -7.,   -8.,  -11.,  -12., -103., -104., -107., -108.,\n",
            "          -111., -112.],\n",
            "         [ -13.,  -14.,  -17.,  -18.,  -21.,  -22., -113., -114., -117., -118.,\n",
            "          -121., -122.],\n",
            "         [ -15.,  -16.,  -19.,  -20.,  -23.,  -24., -115., -116., -119., -120.,\n",
            "          -123., -124.]]], device='cuda:0')\n",
            "\n",
            "p_spatial.shape\n",
            " torch.Size([2, 4, 12])\n",
            "PARFAIT\n",
            "\n",
            " =====preparation pour travail temporel====\n",
            "\n",
            "tensor([[[[   1.,  101.,   -1., -101.],\n",
            "          [   2.,  102.,   -2., -102.]],\n",
            "\n",
            "         [[   5.,  105.,   -5., -105.],\n",
            "          [   6.,  106.,   -6., -106.]],\n",
            "\n",
            "         [[   9.,  109.,   -9., -109.],\n",
            "          [  10.,  110.,  -10., -110.]]],\n",
            "\n",
            "\n",
            "        [[[   3.,  103.,   -3., -103.],\n",
            "          [   4.,  104.,   -4., -104.]],\n",
            "\n",
            "         [[   7.,  107.,   -7., -107.],\n",
            "          [   8.,  108.,   -8., -108.]],\n",
            "\n",
            "         [[  11.,  111.,  -11., -111.],\n",
            "          [  12.,  112.,  -12., -112.]]],\n",
            "\n",
            "\n",
            "        [[[  13.,  113.,  -13., -113.],\n",
            "          [  14.,  114.,  -14., -114.]],\n",
            "\n",
            "         [[  17.,  117.,  -17., -117.],\n",
            "          [  18.,  118.,  -18., -118.]],\n",
            "\n",
            "         [[  21.,  121.,  -21., -121.],\n",
            "          [  22.,  122.,  -22., -122.]]],\n",
            "\n",
            "\n",
            "        [[[  15.,  115.,  -15., -115.],\n",
            "          [  16.,  116.,  -16., -116.]],\n",
            "\n",
            "         [[  19.,  119.,  -19., -119.],\n",
            "          [  20.,  120.,  -20., -120.]],\n",
            "\n",
            "         [[  23.,  123.,  -23., -123.],\n",
            "          [  24.,  124.,  -24., -124.]]]], device='cuda:0')\n",
            "\n",
            "p_temp.shape\n",
            " torch.Size([4, 3, 2, 4])\n",
            "je ne sais pas si c'est PARFAIT\n"
          ]
        }
      ]
    }
  ]
}