{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Opérations de type vivit\n",
        "\n",
        "L'idée est travailler sur des données de type vidéo et de comprendre comment les manipuler selon les differents modeles de vivit (model1, model2, model3, model4)"
      ],
      "metadata": {
        "id": "9wMHWM-QnqP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Déja, on fait les install et les imports"
      ],
      "metadata": {
        "id": "d40F56BmoQ3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyBnVN2RepmM",
        "outputId": "25dc275b-969b-4794-9d55-8b818d17d8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchviz) (2.9.0+cu126)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchviz) (3.0.3)\n",
            "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "import einops\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "iEtoKo0Ae1aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Manipulations spatio temporelles sur des videos\n",
        "\n",
        "A partir d'ici, on a compris que si on rajoute une dimension Batch, les opérations se feront sans difficultés sur chacun des items du batch par brodcasting.\n",
        "\n",
        "On ne travaillera donc plus, pour ces démos, avec des données batchées.\n",
        "\n",
        "#### Préparation d'une vidéo exemple\n",
        "\n",
        "Pour simplifier, on n'extrait plus de patchs, on va directement créer des patch vectorisés avec des tailles réduites qui nous arrangent\n",
        "\n",
        "- un patch a un embedding de taille 4 (eventuellement 2x1x2)\n",
        "- pour un indice temporel, on a 3 patchs spatiaux differents (eventuellement 3x1)\n",
        "- on a 2 indices temporels.\n",
        "\n",
        "Nos données ont une shape : $(npt,nph \\times npw, embed)$ = [2,3,4]"
      ],
      "metadata": {
        "id": "UIF-jAZzU8yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 trucs inutiles dans la pratique ca donne juste la longueur de l'embedding\n",
        "hp = 1\n",
        "wp = 2\n",
        "c = 2\n",
        "embed_dim = hp*wp*c\n",
        "\n",
        "# 2 trucs inutiles, ca sert juste a donner le nombre de patchs spatiaux\n",
        "nph = 3\n",
        "npw = 1\n",
        "\n",
        "nps = nph*npw\n",
        "\n",
        "# nombre de patchs temporels\n",
        "npt = 2\n",
        "\n",
        "# on cree un premier patch\n",
        "p1 = torch.tensor([i+1 for i in range(embed_dim)])\n",
        "\n",
        "print(\"p1\\n\",p1)\n",
        "\n",
        "# on crée les différents patchs spatiaux pour p1\n",
        "list_ps1 = [p1*(10**(i)) for i in range(nps)]\n",
        "\n",
        "ps1 = torch.stack(list_ps1)\n",
        "\n",
        "patches = einops.rearrange([ps1, -ps1],\"npt nps embed_dim-> npt nps embed_dim\")\n",
        "\n",
        "patches = einops.rearrange(patches,\"npt nps embed_dim-> npt nps embed_dim\")\n",
        "\n",
        "print (\"\\npatches\\n\", patches)\n",
        "print (\"\\npatches\\n\", patches.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDkOtqJ8VGcz",
        "outputId": "89ef24ae-c7eb-4fb2-f9a8-ee421098bcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p1\n",
            " tensor([1, 2, 3, 4])\n",
            "\n",
            "patches\n",
            " tensor([[[   1,    2,    3,    4],\n",
            "         [  10,   20,   30,   40],\n",
            "         [ 100,  200,  300,  400]],\n",
            "\n",
            "        [[  -1,   -2,   -3,   -4],\n",
            "         [ -10,  -20,  -30,  -40],\n",
            "         [-100, -200, -300, -400]]])\n",
            "\n",
            "patches\n",
            " torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encodage et traitement spatial, ou temporel, ou les deux\n",
        "\n",
        "\n",
        "Dans ce qui précède, on a pris nos données, encodé chaque patch (tubelet) en vecteurs, et les différents tubelets sont encodés en vecteurs. J'ai donc une matrice pour la vidéo.\n",
        "\n",
        "pour ne pas se mélanger, voici la shape de nos données : $[npt ,(nph \\times npw), (embed)]$\n",
        "\n",
        "Nos traitements vont consister à appliquer une attention bidon à nos données.\n",
        "\n",
        "une **attention** va consister à ajouter à chaque token un vecteur dépendant des données considérées. par exemple, **le max**.\n",
        "\n"
      ],
      "metadata": {
        "id": "JyLHu6wVkX1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(x):\n",
        "  maxi,i_maxi = torch.max(x,dim=-2)\n",
        "  return torch.unsqueeze(maxi,-2)"
      ],
      "metadata": {
        "id": "Tp36RRPkMuyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Travail spatio temporel\n",
        "\n",
        "Pour un travail \"spatio-temporel\", au sens du **model 1 des vivit** : les transformers calculent l'attention entre tous les patchs (spatiaux et temporels)\n",
        "\n",
        "\n",
        "**il suffit de fusionner les dimensions correspondant à nt et ns**.\n",
        "\n",
        "- traitement : vu nos données, on va ajouter à chaque token le vecteur $[100,200,300,400]$."
      ],
      "metadata": {
        "id": "yXBIuRJi2uqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n=====preparation pour travail spatiotemporel=====\\n\")\n",
        "print(\"\\n patches \\n\",patches)\n",
        "print(\"\\n patches.shape \\n\",patches.shape)\n",
        "\n",
        "p_spatio_temp = einops.rearrange(patches,\"npt nps embed -> (npt nps) embed\")\n",
        "print(\"\\np_spatio_temp\\n\",p_spatio_temp)\n",
        "print(\"\\np_spatio_temp.shape\\n\",p_spatio_temp.shape)\n",
        "n_patch,embed_dim = p_spatio_temp.shape\n",
        "\n",
        "#att_spatio_temp = torch.eye(embed_dim,embed_dim)*1.1\n",
        "#print (att_spatio_temp)\n",
        "#print (\"att_spatio_temp.shape\",att_spatio_temp.shape)\n",
        "\n",
        "maxi = attention(p_spatio_temp)\n",
        "print(\"\\n maxi.shape\\n\",maxi.shape)\n",
        "print(\"\\n maxi\\n\",maxi)\n",
        "res = p_spatio_temp + maxi\n",
        "print(\"\\n res\\n\",res)\n",
        "print(\"\\n res.shape\",res.shape)\n",
        "print(\" parfait \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb5AU_r524wX",
        "outputId": "1458224c-939e-445f-8351-83bb7c86a2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====preparation pour travail spatiotemporel=====\n",
            "\n",
            "\n",
            " patches \n",
            " tensor([[[   1,    2,    3,    4],\n",
            "         [  10,   20,   30,   40],\n",
            "         [ 100,  200,  300,  400]],\n",
            "\n",
            "        [[  -1,   -2,   -3,   -4],\n",
            "         [ -10,  -20,  -30,  -40],\n",
            "         [-100, -200, -300, -400]]])\n",
            "\n",
            " patches.shape \n",
            " torch.Size([2, 3, 4])\n",
            "\n",
            "p_spatio_temp\n",
            " tensor([[   1,    2,    3,    4],\n",
            "        [  10,   20,   30,   40],\n",
            "        [ 100,  200,  300,  400],\n",
            "        [  -1,   -2,   -3,   -4],\n",
            "        [ -10,  -20,  -30,  -40],\n",
            "        [-100, -200, -300, -400]])\n",
            "\n",
            "p_spatio_temp.shape\n",
            " torch.Size([6, 4])\n",
            "\n",
            " maxi.shape\n",
            " torch.Size([1, 4])\n",
            "\n",
            " maxi\n",
            " tensor([[100, 200, 300, 400]])\n",
            "\n",
            " res\n",
            " tensor([[101, 202, 303, 404],\n",
            "        [110, 220, 330, 440],\n",
            "        [200, 400, 600, 800],\n",
            "        [ 99, 198, 297, 396],\n",
            "        [ 90, 180, 270, 360],\n",
            "        [  0,   0,   0,   0]])\n",
            "\n",
            " res.shape torch.Size([6, 4])\n",
            " parfait \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Travail spatial puis temporel : model 2 vivit (factorized encoder)\n",
        "\n",
        "Dans le cas du **modele 2 des vivit** :\n",
        "- on a un transformer spatial (avec eventuellement plein de couches)\n",
        "- puis on selectionne un représentant de chaque indice temporel\n",
        "- on a un transformer temporel (avec eventuellement plein de couches).\n",
        "\n",
        "\n",
        "**preparation pour traitement spatial**\n",
        "\n",
        "- On traite chaque indice temporel indépendamment. Cet indice temporel contient toutes les dimensions spatiales de ces moments.\n",
        "\n",
        "c'est l'equivalent d'un batch de tous les indices temporels.\n",
        "\n",
        "C'est déja la forme de nos données\n",
        "\n",
        "- le coeur de mon attention est calculé sur une liste de *ns* vecteurs $n_{seq} = (nph \\times npw)$ (n_seq = 2x2 = 4)\n",
        "- chaque vecteur est de taille *embed_dim* : $embed_{dim} = (fp \\times c \\times hp \\times wp)$ (embed_dim = 8x2x3x2 = 24\n",
        "\n",
        "on a $npt$ indices temporels différents => je calcule ca par batch.\n",
        "Le resultat est de taille $npt \\times nseq \\times embed_{dim}$ : 4x4x24\n",
        "\n",
        "ces inputs sont injectés alors dans l'encodeur spatial\n",
        "\n",
        "**sélection d'un représentant d'un indice temporel**\n",
        "\n",
        "Dans ce modele, après avoir modifié tous les patchs en fonction des patchs de meme indice temporel, on selectionne **un seul patch** de chaque indice temporel.\n",
        "\n",
        "Ce peut être :\n",
        "- le premier (**cls**)\n",
        "- un global average.\n",
        "\n",
        "ici, on va prendre le premier, c'est plus facile à visualiser.\n",
        "\n",
        "le résultat de cette sélection est une liste de $npt$ patchs, chacun de taille $embed_dim$, soit 4 patchs de taille 24\n",
        "\n",
        "**traitement temporel**\n",
        "\n",
        "on injecte ces inputs dans le transformeur temporel\n",
        "\n"
      ],
      "metadata": {
        "id": "ozyxkqnj18D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n=====preparation pour travail spatial=====\\n\")\n",
        "\n",
        "p_spatial = patches\n",
        "print(p_spatial)\n",
        "print(\"\\np_spatial.shape\\n\",p_spatial.shape)\n",
        "print(\"PARFAIT\")\n",
        "\n",
        "# on applique la transfo spatiale\n",
        "print(\"\\t ------------ transfo spatiale -------------\")\n",
        "\n",
        "maxi = attention(p_spatial)\n",
        "print(\"\\nmaxi\\n\",maxi)\n",
        "print(\"\\nmaxi.shape\\n\",maxi.shape)\n",
        "res_spatial = p_spatial + maxi\n",
        "\n",
        "print(\"\\nres_spatial\\n\",res_spatial)\n",
        "print(\"\\nres.shape\",res_spatial.shape)\n",
        "print(\"PARFAIT\")\n",
        "\n",
        "# on selectionne le premier token de chaque indice temporel\n",
        "p_temp = res_spatial[:,0]\n",
        "\n",
        "print(\"\\t ------------ selection d'un token spatial par indice temporel  -------------\")\n",
        "print(\"p_temp.shape\",p_temp.shape)\n",
        "print(\"\\np_temp\\n\",p_temp.int())\n",
        "print(\"PARFAIT\")\n",
        "\n",
        "# on applique la transfo temporelle\n",
        "print(\"\\t ------------ transfo temporelle -------------\")\n",
        "maxi = attention(p_temp)\n",
        "print(\"\\nmaxi\\n\",maxi)\n",
        "print(\"\\nmaxi.shape\\n\",maxi.shape)\n",
        "res_temp = p_temp + maxi\n",
        "\n",
        "\n",
        "print(\"res_temp.shape\",res_temp.shape)\n",
        "print(\"\\nres_temp\\n\",res_temp.int())\n",
        "print(\"PARFAIT\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9IHT-t3j4xW",
        "outputId": "9cdbe7fd-bdc1-4e02-9e72-e1b8d379ab5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====preparation pour travail spatial=====\n",
            "\n",
            "tensor([[[   1,    2,    3,    4],\n",
            "         [  10,   20,   30,   40],\n",
            "         [ 100,  200,  300,  400]],\n",
            "\n",
            "        [[  -1,   -2,   -3,   -4],\n",
            "         [ -10,  -20,  -30,  -40],\n",
            "         [-100, -200, -300, -400]]])\n",
            "\n",
            "p_spatial.shape\n",
            " torch.Size([2, 3, 4])\n",
            "PARFAIT\n",
            "\t ------------ transfo spatiale -------------\n",
            "\n",
            "maxi\n",
            " tensor([[[100, 200, 300, 400]],\n",
            "\n",
            "        [[ -1,  -2,  -3,  -4]]])\n",
            "\n",
            "maxi.shape\n",
            " torch.Size([2, 1, 4])\n",
            "\n",
            "res_spatial\n",
            " tensor([[[ 101,  202,  303,  404],\n",
            "         [ 110,  220,  330,  440],\n",
            "         [ 200,  400,  600,  800]],\n",
            "\n",
            "        [[  -2,   -4,   -6,   -8],\n",
            "         [ -11,  -22,  -33,  -44],\n",
            "         [-101, -202, -303, -404]]])\n",
            "\n",
            "res.shape torch.Size([2, 3, 4])\n",
            "PARFAIT\n",
            "\t ------------ selection d'un token spatial par indice temporel  -------------\n",
            "p_temp.shape torch.Size([2, 4])\n",
            "\n",
            "p_temp\n",
            " tensor([[101, 202, 303, 404],\n",
            "        [ -2,  -4,  -6,  -8]], dtype=torch.int32)\n",
            "PARFAIT\n",
            "\t ------------ transfo temporelle -------------\n",
            "\n",
            "maxi\n",
            " tensor([[101, 202, 303, 404]])\n",
            "\n",
            "maxi.shape\n",
            " torch.Size([1, 4])\n",
            "res_temp.shape torch.Size([2, 4])\n",
            "\n",
            "res_temp\n",
            " tensor([[202, 404, 606, 808],\n",
            "        [ 99, 198, 297, 396]], dtype=torch.int32)\n",
            "PARFAIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### block = encodeur spatial puis encodeur temporel : model 3 **factorized self attention**\n",
        "\n",
        "Dans le model 3 (factorized self attention), un block d'encoder est la succession d'un encodeur spatial et d'un encodeur temporel.\n",
        "\n",
        "pour l'encodeur spatial, c'est la meme chose que celui d'avant.\n",
        "\n",
        "pour l'encodeur temporel, on calcule, pour chaque token spatial, l'attention entre tous les token de meme position, mais d'indice temporel différent.\n",
        "\n",
        "On va deja coder ca\n",
        "\n"
      ],
      "metadata": {
        "id": "F6kUjSSHazCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n =====preparation pour travail temporel====\\n\")\n",
        "# shape avant : [ni_patch * nj_patch, frames,C,Hp,Wp]\n",
        "p_temp = einops.rearrange(patches,\"npt nps embed -> nps npt embed \")\n",
        "print(p_temp)\n",
        "print(\"\\np_temp.shape\\n\",p_temp.shape)\n",
        "\n",
        "# on applique la transfo temporelle\n",
        "print(\"\\t ------------ transfo temporelle -------------\")\n",
        "maxi = attention(p_temp)\n",
        "print(\"\\nmaxi\\n\",maxi)\n",
        "print(\"\\nmaxi.shape\\n\",maxi.shape)\n",
        "res_temp = p_temp + maxi\n",
        "\n",
        "print(\"res_temp.shape\",res_temp.shape)\n",
        "print(\"\\nres_temp\\n\",res_temp.int())\n",
        "print(\"PARFAIT\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xg-ay1hz9I0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16500380-c3dc-4114-e9e3-1ab2c446859d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " =====preparation pour travail temporel====\n",
            "\n",
            "tensor([[[   1,    2,    3,    4],\n",
            "         [  -1,   -2,   -3,   -4]],\n",
            "\n",
            "        [[  10,   20,   30,   40],\n",
            "         [ -10,  -20,  -30,  -40]],\n",
            "\n",
            "        [[ 100,  200,  300,  400],\n",
            "         [-100, -200, -300, -400]]])\n",
            "\n",
            "p_temp.shape\n",
            " torch.Size([3, 2, 4])\n",
            "\t ------------ transfo temporelle -------------\n",
            "\n",
            "maxi\n",
            " tensor([[[  1,   2,   3,   4]],\n",
            "\n",
            "        [[ 10,  20,  30,  40]],\n",
            "\n",
            "        [[100, 200, 300, 400]]])\n",
            "\n",
            "maxi.shape\n",
            " torch.Size([3, 1, 4])\n",
            "res_temp.shape torch.Size([3, 2, 4])\n",
            "\n",
            "res_temp\n",
            " tensor([[[  2,   4,   6,   8],\n",
            "         [  0,   0,   0,   0]],\n",
            "\n",
            "        [[ 20,  40,  60,  80],\n",
            "         [  0,   0,   0,   0]],\n",
            "\n",
            "        [[200, 400, 600, 800],\n",
            "         [  0,   0,   0,   0]]], dtype=torch.int32)\n",
            "PARFAIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**realisation d'un bloc encoder model3**\n",
        "\n",
        "Vu que ce qui précède fonctionne, on peut maintenant chainer les 2 :\n",
        "1. encodeur spatial\n",
        "2. permutations pour encodeur spatial\n",
        "3. encodeur temporel\n",
        "4. permutation pour passer à la suite.\n"
      ],
      "metadata": {
        "id": "YacE_0JmutRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=====preparation pour travail spatial=====\\n\")\n",
        "\n",
        "p_spatial = patches\n",
        "print(p_spatial)\n",
        "print(\"\\np_spatial.shape\\n\",p_spatial.shape)\n",
        "\n",
        "# on applique la transfo spatiale\n",
        "print(\"\\t ------------ transfo spatiale -------------\")\n",
        "\n",
        "maxi = attention(p_spatial)\n",
        "print(\"\\nmaxi\\n\",maxi)\n",
        "print(\"\\nmaxi.shape\\n\",maxi.shape)\n",
        "res_spatial = p_spatial + maxi\n",
        "\n",
        "print(\"\\nres_spatial\\n\",res_spatial)\n",
        "print(\"\\nres.shape\",res_spatial.shape)\n",
        "\n",
        "print(\"\\n =====preparation pour travail temporel====\\n\")\n",
        "# shape avant : [ni_patch * nj_patch, frames,C,Hp,Wp]\n",
        "p_temp = einops.rearrange(res_spatial,\"npt nps embed -> nps npt embed \")\n",
        "print(\"\\ninput de l'attention temporelle\\n\",p_temp)\n",
        "print(\"\\np_temp.shape\\n\",p_temp.shape)\n",
        "\n",
        "# on applique la transfo temporelle\n",
        "print(\"\\t ------------ transfo temporelle -------------\")\n",
        "maxi = attention(p_temp)\n",
        "print(\"\\nmaxi\\n\",maxi)\n",
        "print(\"\\nmaxi.shape\\n\",maxi.shape)\n",
        "res_temp = p_temp + maxi\n",
        "\n",
        "print(\"res_temp.shape\",res_temp.shape)\n",
        "print(\"\\nres_temp\\n\",res_temp.int())\n",
        "\n",
        "print(\"\\n =====preparation pour poursuite====\\n\")\n",
        "out = einops.rearrange(res_temp,\"nps npt embed -> npt nps embed \")\n",
        "print(\"\\nout\\n\",out)"
      ],
      "metadata": {
        "id": "fzpmVMtRb08D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1e8d3b-3a4e-4736-b16a-dc5ea7746694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====preparation pour travail spatial=====\n",
            "\n",
            "tensor([[[   1,    2,    3,    4],\n",
            "         [  10,   20,   30,   40],\n",
            "         [ 100,  200,  300,  400]],\n",
            "\n",
            "        [[  -1,   -2,   -3,   -4],\n",
            "         [ -10,  -20,  -30,  -40],\n",
            "         [-100, -200, -300, -400]]])\n",
            "\n",
            "p_spatial.shape\n",
            " torch.Size([2, 3, 4])\n",
            "\t ------------ transfo spatiale -------------\n",
            "\n",
            "maxi\n",
            " tensor([[[100, 200, 300, 400]],\n",
            "\n",
            "        [[ -1,  -2,  -3,  -4]]])\n",
            "\n",
            "maxi.shape\n",
            " torch.Size([2, 1, 4])\n",
            "\n",
            "res_spatial\n",
            " tensor([[[ 101,  202,  303,  404],\n",
            "         [ 110,  220,  330,  440],\n",
            "         [ 200,  400,  600,  800]],\n",
            "\n",
            "        [[  -2,   -4,   -6,   -8],\n",
            "         [ -11,  -22,  -33,  -44],\n",
            "         [-101, -202, -303, -404]]])\n",
            "\n",
            "res.shape torch.Size([2, 3, 4])\n",
            "\n",
            " =====preparation pour travail temporel====\n",
            "\n",
            "\n",
            "input de l'attention temporelle\n",
            " tensor([[[ 101,  202,  303,  404],\n",
            "         [  -2,   -4,   -6,   -8]],\n",
            "\n",
            "        [[ 110,  220,  330,  440],\n",
            "         [ -11,  -22,  -33,  -44]],\n",
            "\n",
            "        [[ 200,  400,  600,  800],\n",
            "         [-101, -202, -303, -404]]])\n",
            "\n",
            "p_temp.shape\n",
            " torch.Size([3, 2, 4])\n",
            "\t ------------ transfo temporelle -------------\n",
            "\n",
            "maxi\n",
            " tensor([[[101, 202, 303, 404]],\n",
            "\n",
            "        [[110, 220, 330, 440]],\n",
            "\n",
            "        [[200, 400, 600, 800]]])\n",
            "\n",
            "maxi.shape\n",
            " torch.Size([3, 1, 4])\n",
            "res_temp.shape torch.Size([3, 2, 4])\n",
            "\n",
            "res_temp\n",
            " tensor([[[ 202,  404,  606,  808],\n",
            "         [  99,  198,  297,  396]],\n",
            "\n",
            "        [[ 220,  440,  660,  880],\n",
            "         [  99,  198,  297,  396]],\n",
            "\n",
            "        [[ 400,  800, 1200, 1600],\n",
            "         [  99,  198,  297,  396]]], dtype=torch.int32)\n",
            "\n",
            " =====preparation pour poursuite====\n",
            "\n",
            "\n",
            "out\n",
            " tensor([[[ 202,  404,  606,  808],\n",
            "         [ 220,  440,  660,  880],\n",
            "         [ 400,  800, 1200, 1600]],\n",
            "\n",
            "        [[  99,  198,  297,  396],\n",
            "         [  99,  198,  297,  396],\n",
            "         [  99,  198,  297,  396]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 4: Factorised dot-product attention\n",
        "\n",
        "Bon, là, faut relire\n"
      ],
      "metadata": {
        "id": "LAGxJlBb6kHF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7u7IGQS7LVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}